{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018aa53d-025f-402c-ac33-a14df2ea888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/orpheus_tts/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd81f7b-610a-476a-8b1d-e60b713ec097",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=GROQ_API_KEY,\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    model_name=\"groq/compound-mini\",\n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03280ede-cd23-4037-a286-d8d27c325bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have! How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"hello how are you?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37a03980-54e5-4302-8787-6d1a5f7414b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector embeddings are a way to represent complex data, like words or images, as simple numerical vectors in a high-dimensional space, allowing similar things to be close together.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a assisstant who just gives answers in 1 sentence\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "resp = chain.invoke({\"question\": \"Explain vector embeddings in simple terms.\"})\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ee4f6-0f2c-4e19-948b-4337c8b2e3b5",
   "metadata": {},
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d06611-e5df-4a60-b07c-2c099a439a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text you provided appears to be a JSON (JavaScript Object Notation) object containing metadata about a response generated by a large language model. If you'd like, I can help extract and translate the primary content.\n",
      "\n",
      "The primary content is:\n",
      "\n",
      "\"LangChain simplifies the development of Large Language Model (LLM) applications.\"\n",
      "\n",
      "Here's the translation to Kannada:\n",
      "\n",
      "\"ಲಾಂಗ್‌ಚೈನ್ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿ (ಎಲ್‌ಎಲ್‌ಎಂ) ಅಪ್ಲಿಕೇಶನ್‌ಗಳ ಅಭಿವೃದ್ಧಿಯನ್ನು ಸರಳಗೊಳಿಸುತ್ತದೆ.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Step 1: Summarize\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Summarize this text in one sentence: {input_text}\")\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "# Step 2: Translate\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Translate this text to kannada: {summary}\")\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "# Combine them with mapping\n",
    "overall_chain = (\n",
    "    {\"summary\": chain1}  # first step outputs {summary: ...}\n",
    "    | chain2             # second step consumes it\n",
    ")\n",
    "\n",
    "resp = overall_chain.invoke({\"input_text\": \"LangChain makes building LLM apps easier.\"})\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b3410-9b67-4c3f-9713-dc185076a33c",
   "metadata": {},
   "source": [
    "# Rember conversation and chatbot style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0196ac90-efb2-4f2d-895c-3fd302870464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU: hello, i'm lavanya\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Hello Lavanya! It's nice to meet you. I'm Compound-Beta, a conversational AI built by Groq. I'm running on a custom LPU (Language Processing Unit) hardware designed for fast AI inference. I'm excited to chat with you and share my knowledge. How's your day going so far?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU: i'm 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Happy 27th birthday, Lavanya! I'm glad I got to know your name and now your age. I'm still learning and growing, but I'm not aware of having a birthday myself since I'm a software program. I'm always here and ready to chat, though! What do you like to do for fun, or is there something specific on your mind that you'd like to talk about? By the way, I'm processing our conversation using my LPU hardware, which allows me to respond quickly and efficiently. It's a 16-core LPU, and it's capable of handling a large number of conversations simultaneously.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU: whats my age\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: You mentioned earlier that you're 27 years old, Lavanya!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "YOU: exit\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "while True:\n",
    "    user1 = input(\"YOU:\")\n",
    "    if user1 == \"exit\":\n",
    "        break\n",
    "    response = conversation.predict(input=user1)\n",
    "    print(\"BOT:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754a5d8-9c1a-48cf-8643-a3cd43e468cc",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c223ad2-de77-454c-b531-223ac8e63bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 1\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"/Users/lavanya/Downloads/Lavanya_SWE.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"Loaded pages:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f1d7e63-8563-4e2e-8ce9-25179e7713f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lavanya Shankar\\nlavanyashankar'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51a85004-a80f-4233-bcd6-46dab22430bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 10\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da4c2e22-9b26-46f7-ab3a-43497ef20796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bv/c7b6nvgj05s7ftxp8534dp0w0000gn/T/ipykernel_92332/810526889.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee2fe74-df65-4e03-828d-cec96be9b3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x303323e10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5f47a4e-acb6-4cab-bb3d-425dab5dfdf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks stored: 10\n",
      "Stored text: Lavanya Shankar\n",
      "lavanyashankarsv09@gmail.com| +1 704-490-9869 | LinkedIn | GitHub | Medium | Portfolio\n",
      "Education\n",
      "Johns Hopkins University Baltimore, MD\n",
      "Master of Science in Engineering in Data Science\n",
      "Embedding length: 384\n",
      "First 10 values: [-0.0776967  -0.01542668 -0.01383872  0.01303261 -0.04081499 -0.09348963\n",
      " -0.04556158  0.00163316 -0.1418069  -0.00174948]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of chunks stored:\", db._collection.count())\n",
    "# Peek into the DB\n",
    "raw = db._collection.get(include=[\"embeddings\", \"documents\"], limit=1)\n",
    "print(\"Stored text:\", raw[\"documents\"][0][:200])\n",
    "print(\"Embedding length:\", len(raw[\"embeddings\"][0]))\n",
    "print(\"First 10 values:\", raw[\"embeddings\"][0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a39e6-4be3-48c3-918e-35eb6950c2c5",
   "metadata": {},
   "source": [
    "# Generate 1 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c76dd903-7e64-4b00-ae8e-4302eb9a1060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk text: Lavanya Shankar\n",
      "lavanyashankarsv09@gmail.com| +1 704-490-9869 | LinkedIn | GitHub | Medium | Portfolio\n",
      "Education\n",
      "Johns Hopkins University Baltimore, MD\n",
      "Master of Science in Engineering in Data Science\n",
      "Embedding length: 384\n",
      "First 10 values: [-0.07769669592380524, -0.015426683239638805, -0.013838724233210087, 0.013032610528171062, -0.04081499204039574, -0.0934896320104599, -0.04556158185005188, 0.0016331623774021864, -0.14180688560009003, -0.0017494767671450973]\n"
     ]
    }
   ],
   "source": [
    "# Take the first chunk of your text\n",
    "first_chunk_text = chunks[0].page_content\n",
    "print(\"First chunk text:\", first_chunk_text[:200])  # preview 200 chars\n",
    "\n",
    "# Create embedding\n",
    "first_embedding = embeddings.embed_query(first_chunk_text)\n",
    "print(\"Embedding length:\", len(first_embedding))   # should be 384\n",
    "print(\"First 10 values:\", first_embedding[:10])   # preview first 10 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04396ec5-d940-414e-9d2c-061d1ca7600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Lavanya Shankar's education includes:\n",
      "\n",
      "1. **Master of Science in Engineering in Data Science** at Johns Hopkins University, Baltimore, MD, with a CGPA of 3.9/4 from August 2023 to May 2025.\n",
      "2. **Bachelor of Engineering in Computer Science** at Visvesvaraya Technological University, Bengaluru, India, with a CGPA of 3.6/4 from August 2016 to August 2020.\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Example queries\n",
    "result = qa({\"query\": \"her education\"})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314d8ce-d589-4311-8c26-b494b2fefced",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ed85ec8-3800-48e4-9281-63797ee21245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Match 1 ---\n",
      "Text: Lavanya Shankar\n",
      "lavanyashankarsv09@gmail.com| +1 704-490-9869 | LinkedIn | GitHub | Medium | Portfolio\n",
      "Education\n",
      "Johns Hopkins University Baltimore, MD\n",
      "Master of Science in Engineering in Data Science | CGPA: 3.9/4 Aug 2023 – May 2025\n",
      "Visvesvaraya Technological University Bengaluru, India\n",
      "Bachelor of Engineering in Computer Science | CGPA: 3.6/4 Aug 2016 – Aug 2020\n",
      "Technical Skills\n",
      "Programming: Python, Java, JavaScript, Ansible, SQL, HTML/CSS, Spring Boot, C/C++\n",
      "\n",
      "--- Match 2 ---\n",
      "Text: ACL 2025: Generated educational material for 4 low-resource Indigenous languages employing POS tagging,\n",
      "chain-of-thought reasoning, and ensemble learning; improved accuracy by 10% over previous benchmarks (ACL Paper )\n",
      "ACL 2025: Created spoken language translation systems for 10 language pairs leveraging SeamlessM4T, Whisper, and\n",
      "Whisper+NLLB models; applied Minimum Bayes Risk (MBR) ensembling to enhance translation accuracy (ACL Paper )\n",
      "Projects\n",
      "Aragorn - Agentic RAG Bot Link\n"
     ]
    }
   ],
   "source": [
    "query = \"Lavanya’s education\"\n",
    "results = db.similarity_search(query, k=2)  # top 3 matches\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n--- Match {i+1} ---\")\n",
    "    print(\"Text:\", r.page_content)\n",
    "    #print(\"Metadata:\", r.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43608418-911b-4289-a305-7d4c8ba6619c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orpheus TTS",
   "language": "python",
   "name": "orpheus_tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
